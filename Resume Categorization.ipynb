{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Resume Categorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Resume Cateorization](CreatingaPhysicianCVthatShines.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project tackles the challenge of **resume categorization** using machine learning and deep learning techniques. Companies often face the daunting task of sifting through numerous resumes for each job opening. This app aims to automate and streamline this process by predicting the job category a given resume belongs to. By using a trained model, the app can quickly suggest the appropriate job category for each resume, saving time and resources for recruiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Set Environmnt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading The Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Acquisition:** The project uses the [Resume Dataset](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) from Kaggle. This dataset consists of resumes categorized into 25 distinct job fields, providing a solid foundation for training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                             Resume\n",
       "0  Data Science  Skills * Programming Languages: Python (pandas...\n",
       "1  Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...\n",
       "2  Data Science  Areas of Interest Deep Learning, Control Syste...\n",
       "3  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n",
       "4  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeDataSet = pd.read_csv(\"E:\\\\ERU\\\\Level 4\\\\S1\\\\ML\\\\Project\\\\UpdatedResumeDataSet.csv\\\\UpdatedResumeDataSet.csv\")\n",
    "resumeDataSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Displaying the distinct categories of resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the distinct categories of resume:\n",
      "\n",
      " \n",
      "['Data Science' 'HR' 'Advocate' 'Arts' 'Web Designing'\n",
      " 'Mechanical Engineer' 'Sales' 'Health and fitness' 'Civil Engineer'\n",
      " 'Java Developer' 'Business Analyst' 'SAP Developer' 'Automation Testing'\n",
      " 'Electrical Engineering' 'Operations Manager' 'Python Developer'\n",
      " 'DevOps Engineer' 'Network Security Engineer' 'PMO' 'Database' 'Hadoop'\n",
      " 'ETL Developer' 'DotNet Developer' 'Blockchain' 'Testing']\n"
     ]
    }
   ],
   "source": [
    "print (\"Displaying the distinct categories of resume:\\n\\n \")\n",
    "print (resumeDataSet['Category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Displaying the number of resumes in each category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the distinct categories of resume and the number of records belonging to each category:\n",
      "\n",
      "\n",
      "Category\n",
      "Java Developer               84\n",
      "Testing                      70\n",
      "DevOps Engineer              55\n",
      "Python Developer             48\n",
      "Web Designing                45\n",
      "HR                           44\n",
      "Hadoop                       42\n",
      "Sales                        40\n",
      "Data Science                 40\n",
      "Mechanical Engineer          40\n",
      "ETL Developer                40\n",
      "Blockchain                   40\n",
      "Operations Manager           40\n",
      "Arts                         36\n",
      "Database                     33\n",
      "Health and fitness           30\n",
      "PMO                          30\n",
      "Electrical Engineering       30\n",
      "Business Analyst             28\n",
      "DotNet Developer             28\n",
      "Automation Testing           26\n",
      "Network Security Engineer    25\n",
      "Civil Engineer               24\n",
      "SAP Developer                24\n",
      "Advocate                     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Displaying the distinct categories of resume and the number of records belonging to each category:\\n\\n\")\n",
    "print (resumeDataSet['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeDataSet['Category'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Exprience - 24 months\\r\\njQuery- Exprience - 24 months\\r\\nPython- Exprience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIEW\\r\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\\r\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to review all red flag cases.\\r\\nâ\\x80¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeDataSet['Resume'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Preprocessing:** Raw resume text is often messy. We apply preprocessing techniques to clean the resume data, including:\n",
    "- Removing URLs, RTs, hashtags, and mentions\n",
    "- Eliminating special characters and non-ASCII characters\n",
    "- Collapsing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt) # This line removes any URLs from the text\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText) # This line removes any RTs or cc from the text\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText) # This line removes any hashtags from the text\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText) # This line removes any @ from the text\n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText) # This line removes any punctuations from the text\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) # This line removes any non-ASCII characters from the text\n",
    "    cleanText = re.sub('\\s+', ' ', cleanText) # This line removes any extra whitespaces from the text\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeDataSet['Resume'] = resumeDataSet['Resume'].apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check cleaned text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skills Programming Languages Python pandas numpy scipy scikit learn matplotlib Sql Java JavaScript JQuery Machine learning Regression SVM Na ve Bayes KNN Random Forest Decision Trees Boosting techniques Cluster Analysis Word Embedding Sentiment Analysis Natural Language processing Dimensionality reduction Topic Modelling LDA NMF PCA Neural Nets Database Visualizations Mysql SqlServer Cassandra Hbase ElasticSearch D3 js DC js Plotly kibana matplotlib ggplot Tableau Others Regular Expression HTML CSS Angular 6 Logstash Kafka Python Flask Git Docker computer vision Open CV and understanding of Deep learning Education Details Data Science Assurance Associate Data Science Assurance Associate Ernst Young LLP Skill Details JAVASCRIPT Exprience 24 months jQuery Exprience 24 months Python Exprience 24 monthsCompany Details company Ernst Young LLP description Fraud Investigations and Dispute Services Assurance TECHNOLOGY ASSISTED REVIEW TAR Technology Assisted Review assists in a elerating the review process and run analytics and generate reports Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain this tool implements predictive coding and topic modelling by automating reviews resulting in reduced labor costs and time spent during the lawyers review Understand the end to end flow of the solution doing research and development for classification models predictive analysis and mining of the information present in text data Worked on analyzing the outputs and precision monitoring for the entire tool TAR assists in predictive coding topic modelling from the evidence by following EY standards Developed the classifier models in order to identify red flags and fraud related issues Tools Technologies Python scikit learn tfidf word2vec doc2vec cosine similarity Na ve Bayes LDA NMF for topic modelling Vader and text blob for sentiment analysis Matplot lib Tableau dashboard for reporting MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS USA CLIENTS TEXT ANALYTICS MOTOR VEHICLE CUSTOMER REVIEW DATA Received customer feedback survey data for past one year Performed sentiment Positive Negative Neutral and time series analysis on customer comments across all 4 categories Created heat map of terms by survey category based on frequency of words Extracted Positive and Negative words across all the Survey categories and plotted Word cloud Created customized tableau dashboards for effective reporting and visualizations CHATBOT Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation reservation options and so on This chat bot serves entire product related questions Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant recommended questions Tools Technologies Python Natural language processing NLTK spacy topic modelling Sentiment analysis Word Embedding scikit learn JavaScript JQuery SqlServer INFORMATION GOVERNANCE Organizations to make informed decisions about all of the information they store The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk Scan data from multiple sources of formats and parse different file formats extract Meta data information push results for indexing elastic search and created customized interactive dashboards using kibana Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant Outdated or Trivial Preforming full text search analysis on elastic search with predefined methods which can tag as PII personally identifiable information social security numbers addresses names etc which frequently targeted during cyber attacks Tools Technologies Python Flask Elastic Search Kibana FRAUD ANALYTIC PLATFORM Fraud Analytics and investigative platform to review all red flag cases FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems It can be used by clients to interrogate their A ounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics Tools Technologies HTML JavaScript SqlServer JQuery CSS Bootstrap Node js D3 js DC js'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeDataSet['Resume'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Data Preparation:**\n",
    "- **Category Encoding:** We transform the textual categories into numerical labels using Label Encoding, allowing our machine learning algorithms to work with the data.\n",
    "- **TF-IDF Vectorization:** We convert the cleaned text data into numerical vectors using TF-IDF (Term Frequency-Inverse Document Frequency), which gives more weight to words that are specific to a document in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Category Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(resumeDataSet['Category'])\n",
    "resumeDataSet['Category'] = le.transform(resumeDataSet['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 12,  0,  1, 24, 16, 22, 14,  5, 15,  4, 21,  2, 11, 18, 20,  8,\n",
       "       17, 19,  7, 13, 10,  9,  3, 23])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumeDataSet.Category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf.fit(resumeDataSet['Resume'])\n",
    "requredTaxt  = tfidf.transform(resumeDataSet['Resume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting:** The dataset is divided into training and testing sets to properly evaluate the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 7351)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(requredTaxt, resumeDataSet['Category'], test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 7351)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Development**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Development:** We build and train multiple models:\n",
    " - **Machine Learning Models:**\n",
    "    *   **K-Nearest Neighbors (KNN):** A simple yet effective classification algorithm based on distance between points.\n",
    "    *   **Support Vector Machine (SVC):** A powerful algorithm that finds an optimal hyperplane to separate classes.\n",
    "    *   **Random Forest:** An ensemble method that combines multiple decision trees for more robust predictions.\n",
    "-   **Deep Learning Model:**\n",
    "    *   **Multilayer Perceptron (MLP):** A neural network model with multiple hidden layers to learn complex patterns from the data.\n",
    "-   **Ensemble Model**\n",
    "    *   **Voting Classifier:** Combines the predictions from the machine learning models to achieve more accurate and robust results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that X_train and X_test are dense if they are sparse\n",
    "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' Excellent grasping power in learning new concepts and technology Highly motivated team player with strong work ethics committed to hard work Ability to work and co ordinate in a team effectively Enthusiastic self starter and team player Quick and independent learner Education Details January 2014 Bachelor of Technology Information Technology branch BPUT University January 2010 Diploma Engineering Brahmapur Orissa U C P Engineering School Software Testing Automation Engineer Software Testing Automation Engineer Tech Mahindra Skill Details Company Details company Tech Mahindra description India Duration Oct 2017 Till Date Project Description BT Group plc trading as BT and formerly British Telecom is a British multinational telecommunications holding company with head offices in London United Kingdom I worked for Air Logistics Program under the banner of British Telecom This project handles all the web applications to carry out the whole logistics operation over United Kingdom through various airlines Roles Responsibilities Design and develop framework for the Test scenarios and Test cases Developing automation test scripts on the existing application Executing Test Cases for every new release Involved in running test cases and logging defects through the HPQC tool Involved in formulating test Summary Report Conduct Internal Test Case Peer Reviews Participated in Daily Scrum Meetings Participated in weekly status meetings with the team developers to discuss open issues and communicating with onsite team company Tech Mahindra Pvt Ltd Pune Tech Mahindra description is an Indian multinational company with around 115 000 Employees spread across 90 countries globally Total Experience 2 Years 7 Months Organization Designation Duration company Tech Mahindra description Project Description AT T Inc is an American multinational conglomerate holding company headquartered at Whitacre Tower in Downtown Dallas Texas During my serving as software engineer at AT T I have worked for CSI CAM Common Service Interface team which is responsible for running of AT T s centralised solution hub web application called myatt com Roles Responsibilities Design develop and maintaing Automation Test Scripts and Test cases using Selenium WebDriver and several desktop window automating tool such as Sikuli and AutoIT Executing Test Cases and check the working functionality of the existing application Involved in tracking manging test life cycle and logging defects using JIRA and HPQC ALM Involved in formulating test Summary Report Conduct Internal Test Case Peer Reviews Participated in Daily Scrum Meetings Participated in weekly status meetings with the team developers to discuss open issues and communicating with onsite team company Tech Mahindra Pvt Ltd Pune Tech Mahindra description Till Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8068\\684270427.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1. Train KNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mknn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mknn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Accuracy for the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_train_pred_knn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1385\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1386\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m                 )\n\u001b[0;32m   1388\u001b[0m             ):\n\u001b[1;32m-> 1389\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[1;31m# In cases where individual estimators are very fast to train setting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;31m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;31m# of spawning threads.  See joblib issue #112.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n\u001b[0m\u001b[0;32m    377\u001b[0m             delayed(_fit_binary)(\n\u001b[0;32m    378\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m         iterable_with_config = (\n\u001b[0;32m     74\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         )\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1914\u001b[0m             \u001b[1;31m# If n_jobs==1, run the computation sequentially and return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m             \u001b[1;31m# immediately to avoid overheads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[1;31m# call is interrupted early and that the same instance is immediately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1859\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             )\n\u001b[0;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, X, y, fit_params, classes)\u001b[0m\n\u001b[0;32m     92\u001b[0m             )\n\u001b[0;32m     93\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ConstantPredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1385\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1386\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m                 )\n\u001b[0;32m   1388\u001b[0m             ):\n\u001b[1;32m-> 1389\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[0mensure_all_finite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                 X, y = validate_data(\n\u001b[0m\u001b[0;32m    479\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2957\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2958\u001b[0m                 \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2961\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2964\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         )\n\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1052\u001b[0m                         )\n\u001b[0;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m                 raise ValueError(\n\u001b[0;32m   1058\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ERU\\Level 4\\S1\\ML\\Project\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ' Excellent grasping power in learning new concepts and technology Highly motivated team player with strong work ethics committed to hard work Ability to work and co ordinate in a team effectively Enthusiastic self starter and team player Quick and independent learner Education Details January 2014 Bachelor of Technology Information Technology branch BPUT University January 2010 Diploma Engineering Brahmapur Orissa U C P Engineering School Software Testing Automation Engineer Software Testing Automation Engineer Tech Mahindra Skill Details Company Details company Tech Mahindra description India Duration Oct 2017 Till Date Project Description BT Group plc trading as BT and formerly British Telecom is a British multinational telecommunications holding company with head offices in London United Kingdom I worked for Air Logistics Program under the banner of British Telecom This project handles all the web applications to carry out the whole logistics operation over United Kingdom through various airlines Roles Responsibilities Design and develop framework for the Test scenarios and Test cases Developing automation test scripts on the existing application Executing Test Cases for every new release Involved in running test cases and logging defects through the HPQC tool Involved in formulating test Summary Report Conduct Internal Test Case Peer Reviews Participated in Daily Scrum Meetings Participated in weekly status meetings with the team developers to discuss open issues and communicating with onsite team company Tech Mahindra Pvt Ltd Pune Tech Mahindra description is an Indian multinational company with around 115 000 Employees spread across 90 countries globally Total Experience 2 Years 7 Months Organization Designation Duration company Tech Mahindra description Project Description AT T Inc is an American multinational conglomerate holding company headquartered at Whitacre Tower in Downtown Dallas Texas During my serving as software engineer at AT T I have worked for CSI CAM Common Service Interface team which is responsible for running of AT T s centralised solution hub web application called myatt com Roles Responsibilities Design develop and maintaing Automation Test Scripts and Test cases using Selenium WebDriver and several desktop window automating tool such as Sikuli and AutoIT Executing Test Cases and check the working functionality of the existing application Involved in tracking manging test life cycle and logging defects using JIRA and HPQC ALM Involved in formulating test Summary Report Conduct Internal Test Case Peer Reviews Participated in Daily Scrum Meetings Participated in weekly status meetings with the team developers to discuss open issues and communicating with onsite team company Tech Mahindra Pvt Ltd Pune Tech Mahindra description Till Date'"
     ]
    }
   ],
   "source": [
    "# 1. Train KNeighborsClassifier\n",
    "knn_model = OneVsRestClassifier(KNeighborsClassifier())\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for the training set\n",
    "y_train_pred_knn = knn_model.predict(X_train)\n",
    "train_accuracy_knn = accuracy_score(y_train, y_train_pred_knn)\n",
    "print(f\"Training Accuracy: {train_accuracy_knn:.4f}\")\n",
    "\n",
    "# Accuracy for the test set\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "test_accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\" Testing Accuracy: {test_accuracy_knn:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_knn)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "\n",
      "SVC Results:\n",
      "Testing Accuracy: 0.9948\n",
      "Confusion Matrix:\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   5]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       1.00      1.00      1.00        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00        15\n",
      "          16       1.00      1.00      1.00         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.88      1.00      0.93         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       1.00      1.00      1.00        16\n",
      "          24       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       193\n",
      "   macro avg       0.99      1.00      1.00       193\n",
      "weighted avg       1.00      0.99      0.99       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Train SVC\n",
    "svc_model = OneVsRestClassifier(SVC())\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for the training set\n",
    "y_train_pred_svc = svc_model.predict(X_train)\n",
    "train_accuracy_svc = accuracy_score(y_train, y_train_pred_svc)\n",
    "print(f\"Training Accuracy: {train_accuracy_svc:.4f}\")\n",
    "\n",
    "# Accuracy for the test set\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "test_accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(\"\\nSVC Results:\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_svc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_svc)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_svc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "\n",
      "RandomForestClassifier Results:\n",
      "Testing Accuracy: 0.9948\n",
      "Confusion Matrix:\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   5]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       1.00      1.00      1.00        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00        15\n",
      "          16       1.00      1.00      1.00         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       1.00      1.00      1.00         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       0.94      1.00      0.97        16\n",
      "          24       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       193\n",
      "   macro avg       1.00      1.00      1.00       193\n",
      "weighted avg       1.00      0.99      0.99       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Train RandomForestClassifier\n",
    "rf_model = OneVsRestClassifier(RandomForestClassifier())\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for the training set\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "print(f\"Training Accuracy: {train_accuracy_rf:.4f}\")\n",
    "\n",
    "# Accuracy for the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"\\nRandomForestClassifier Results:\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_rf:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ensemble Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.9948\n",
      "Confusion Matrix:\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16\n",
      "   0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   5]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       1.00      1.00      1.00        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00        15\n",
      "          16       1.00      1.00      1.00         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.88      1.00      0.93         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       1.00      1.00      1.00        16\n",
      "          24       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       193\n",
      "   macro avg       0.99      1.00      1.00       193\n",
      "weighted avg       1.00      0.99      0.99       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create a VotingClassifier with the three models\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('knn', knn_model),\n",
    "    ('svc', svc_model),\n",
    "    ('rf', rf_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the ensemble model\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_ensemble)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_ensemble)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.3595 - loss: 3.1767 - val_accuracy: 0.6753 - val_loss: 2.9307\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8042 - loss: 2.7127 - val_accuracy: 0.8312 - val_loss: 2.0922\n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8990 - loss: 1.6404 - val_accuracy: 0.8961 - val_loss: 1.0710\n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9652 - loss: 0.6920 - val_accuracy: 0.9870 - val_loss: 0.4553\n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.2786 - val_accuracy: 1.0000 - val_loss: 0.2075\n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0966 - val_accuracy: 1.0000 - val_loss: 0.1147\n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0489 - val_accuracy: 1.0000 - val_loss: 0.0714\n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0299 - val_accuracy: 1.0000 - val_loss: 0.0542\n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0207 - val_accuracy: 1.0000 - val_loss: 0.0432\n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0154 - val_accuracy: 1.0000 - val_loss: 0.0363\n",
      "Final Training Accuracy: 1.0000\n",
      "Final Validation Accuracy: 1.0000\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0426 \n",
      "Test Loss: 0.0546\n",
      "Test Accuracy: 0.9948\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       1.00      1.00      1.00        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00        15\n",
      "          16       1.00      1.00      1.00         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.88      1.00      0.93         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       1.00      1.00      1.00        16\n",
      "          24       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99       193\n",
      "   macro avg       0.99      1.00      1.00       193\n",
      "weighted avg       1.00      0.99      0.99       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Define the MLP model\n",
    "model = keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    Dense(64, activation='relu'),  # Hidden layer\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')  # Output layer (number of classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy = history.history['accuracy'][-1]  # Last epoch's training accuracy\n",
    "val_accuracy = history.history['val_accuracy'][-1]  # Last epoch's validation accuracy\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mlp = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RNN with GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.1946 - loss: 5.0641 - val_accuracy: 0.0909 - val_loss: 3.2306\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.0728 - loss: 3.1867 - val_accuracy: 0.1039 - val_loss: 3.2158\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.0737 - loss: 3.1777 - val_accuracy: 0.0909 - val_loss: 3.2337\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.0878 - loss: 3.1283 - val_accuracy: 0.0260 - val_loss: 3.3137\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.0607 - loss: 3.1741 - val_accuracy: 0.1039 - val_loss: 3.2499\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.1005 - loss: 3.1294 - val_accuracy: 0.1039 - val_loss: 3.2593\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.1053 - loss: 3.1052 - val_accuracy: 0.1039 - val_loss: 3.3649\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0999 - loss: 3.1237 - val_accuracy: 0.1039 - val_loss: 3.3554\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0851 - loss: 3.1301 - val_accuracy: 0.1039 - val_loss: 3.3585\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.0947 - loss: 3.1105 - val_accuracy: 0.1039 - val_loss: 3.2559\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.1031 - loss: 3.1030 - val_accuracy: 0.1039 - val_loss: 3.2621\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.1012 - loss: 3.1276 - val_accuracy: 0.1039 - val_loss: 3.2637\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.0997 - loss: 3.1300 - val_accuracy: 0.1039 - val_loss: 3.2579\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.0981 - loss: 3.1153 - val_accuracy: 0.1039 - val_loss: 3.2589\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0899 - loss: 3.1349 - val_accuracy: 0.1039 - val_loss: 3.2634\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.0847 - loss: 3.1199 - val_accuracy: 0.1039 - val_loss: 3.2680\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.1095 - loss: 3.1380 - val_accuracy: 0.1039 - val_loss: 3.2568\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0828 - loss: 3.1609 - val_accuracy: 0.1039 - val_loss: 3.2654\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.0911 - loss: 3.1498 - val_accuracy: 0.1039 - val_loss: 3.2594\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0903 - loss: 3.1435 - val_accuracy: 0.1039 - val_loss: 3.2643\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.0919 - loss: 3.1447 - val_accuracy: 0.1039 - val_loss: 3.2694\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0846 - loss: 3.1560 - val_accuracy: 0.1039 - val_loss: 3.2553\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0822 - loss: 3.1481 - val_accuracy: 0.1039 - val_loss: 3.2582\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.0861 - loss: 3.1415 - val_accuracy: 0.1039 - val_loss: 3.2683\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.0781 - loss: 3.1717 - val_accuracy: 0.1039 - val_loss: 3.2651\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.0943 - loss: 3.1437 - val_accuracy: 0.1039 - val_loss: 3.2612\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0920 - loss: 3.1439 - val_accuracy: 0.1039 - val_loss: 3.2637\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0798 - loss: 3.1344 - val_accuracy: 0.1039 - val_loss: 3.2682\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0777 - loss: 3.1523 - val_accuracy: 0.1039 - val_loss: 3.2621\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0707 - loss: 3.1615 - val_accuracy: 0.1039 - val_loss: 3.2669\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0914 - loss: 3.1462 - val_accuracy: 0.1039 - val_loss: 3.2609\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0975 - loss: 3.1342 - val_accuracy: 0.1039 - val_loss: 3.2644\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0943 - loss: 3.1446 - val_accuracy: 0.1039 - val_loss: 3.2614\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.0983 - loss: 3.1365 - val_accuracy: 0.1039 - val_loss: 3.2618\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0875 - loss: 3.1565 - val_accuracy: 0.1039 - val_loss: 3.2608\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0695 - loss: 3.1515 - val_accuracy: 0.1039 - val_loss: 3.2663\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0784 - loss: 3.1614 - val_accuracy: 0.1039 - val_loss: 3.2588\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0797 - loss: 3.1524 - val_accuracy: 0.1039 - val_loss: 3.2627\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0838 - loss: 3.1417 - val_accuracy: 0.1039 - val_loss: 3.2627\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0905 - loss: 3.1423 - val_accuracy: 0.1039 - val_loss: 3.2634\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.0838 - loss: 3.1482 - val_accuracy: 0.1039 - val_loss: 3.2684\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0714 - loss: 3.1889 - val_accuracy: 0.1039 - val_loss: 3.2585\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0841 - loss: 3.1621 - val_accuracy: 0.1039 - val_loss: 3.2674\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.0750 - loss: 3.1668 - val_accuracy: 0.1039 - val_loss: 3.2603\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.0794 - loss: 3.1447 - val_accuracy: 0.1039 - val_loss: 3.2693\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0996 - loss: 3.1396 - val_accuracy: 0.1039 - val_loss: 3.2614\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0925 - loss: 3.1448 - val_accuracy: 0.1039 - val_loss: 3.2658\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0883 - loss: 3.1648 - val_accuracy: 0.1039 - val_loss: 3.2655\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0851 - loss: 3.1366 - val_accuracy: 0.1039 - val_loss: 3.2643\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0950 - loss: 3.1347 - val_accuracy: 0.1039 - val_loss: 3.2641\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0904 - loss: 3.1366 - val_accuracy: 0.1039 - val_loss: 3.2679\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0901 - loss: 3.1385 - val_accuracy: 0.1039 - val_loss: 3.2535\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.0959 - loss: 3.1315 - val_accuracy: 0.1039 - val_loss: 3.2644\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0651 - loss: 3.1584 - val_accuracy: 0.1039 - val_loss: 3.2632\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0884 - loss: 3.1418 - val_accuracy: 0.1039 - val_loss: 3.2633\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0749 - loss: 3.1505 - val_accuracy: 0.1039 - val_loss: 3.2653\n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.1107 - loss: 3.1243 - val_accuracy: 0.1039 - val_loss: 3.2624\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0930 - loss: 3.1559 - val_accuracy: 0.1039 - val_loss: 3.2659\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0966 - loss: 3.1573 - val_accuracy: 0.1039 - val_loss: 3.2603\n",
      "Epoch 60/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.0890 - loss: 3.1312 - val_accuracy: 0.1039 - val_loss: 3.2719\n",
      "Epoch 61/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0647 - loss: 3.1806 - val_accuracy: 0.1039 - val_loss: 3.2677\n",
      "Epoch 62/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0777 - loss: 3.1458 - val_accuracy: 0.1039 - val_loss: 3.2631\n",
      "Epoch 63/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.0887 - loss: 3.1496 - val_accuracy: 0.1039 - val_loss: 3.2638\n",
      "Epoch 64/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0776 - loss: 3.1537 - val_accuracy: 0.1039 - val_loss: 3.2668\n",
      "Epoch 65/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.1029 - loss: 3.1282 - val_accuracy: 0.1039 - val_loss: 3.2674\n",
      "Epoch 66/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0881 - loss: 3.1405 - val_accuracy: 0.1039 - val_loss: 3.2620\n",
      "Epoch 67/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.0879 - loss: 3.1478 - val_accuracy: 0.1039 - val_loss: 3.2609\n",
      "Epoch 68/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0989 - loss: 3.1383 - val_accuracy: 0.1039 - val_loss: 3.2694\n",
      "Epoch 69/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.0671 - loss: 3.1720 - val_accuracy: 0.1039 - val_loss: 3.2593\n",
      "Epoch 70/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.1038 - loss: 3.1265 - val_accuracy: 0.1039 - val_loss: 3.2685\n",
      "Epoch 71/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.0696 - loss: 3.1337 - val_accuracy: 0.1039 - val_loss: 3.2677\n",
      "Epoch 72/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0918 - loss: 3.1493 - val_accuracy: 0.1039 - val_loss: 3.2534\n",
      "Epoch 73/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.1098 - loss: 3.1212 - val_accuracy: 0.1039 - val_loss: 3.2754\n",
      "Epoch 74/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0910 - loss: 3.1390 - val_accuracy: 0.1039 - val_loss: 3.2610\n",
      "Epoch 75/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0911 - loss: 3.1407 - val_accuracy: 0.1039 - val_loss: 3.2568\n",
      "Epoch 76/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.1081 - loss: 3.1363 - val_accuracy: 0.1039 - val_loss: 3.2622\n",
      "Epoch 77/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0894 - loss: 3.1594 - val_accuracy: 0.1039 - val_loss: 3.2602\n",
      "Epoch 78/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0890 - loss: 3.1472 - val_accuracy: 0.1039 - val_loss: 3.2691\n",
      "Epoch 79/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0816 - loss: 3.1535 - val_accuracy: 0.1039 - val_loss: 3.2649\n",
      "Epoch 80/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0963 - loss: 3.1393 - val_accuracy: 0.1039 - val_loss: 3.2685\n",
      "Epoch 81/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0825 - loss: 3.1526 - val_accuracy: 0.1039 - val_loss: 3.2598\n",
      "Epoch 82/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0756 - loss: 3.1664 - val_accuracy: 0.1039 - val_loss: 3.2664\n",
      "Epoch 83/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.1009 - loss: 3.1262 - val_accuracy: 0.1039 - val_loss: 3.2697\n",
      "Epoch 84/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.0856 - loss: 3.1535 - val_accuracy: 0.1039 - val_loss: 3.2599\n",
      "Epoch 85/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.0875 - loss: 3.1383 - val_accuracy: 0.1039 - val_loss: 3.2597\n",
      "Epoch 86/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.0953 - loss: 3.1400 - val_accuracy: 0.1039 - val_loss: 3.2620\n",
      "Epoch 87/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0904 - loss: 3.1527 - val_accuracy: 0.1039 - val_loss: 3.2720\n",
      "Epoch 88/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0764 - loss: 3.1476 - val_accuracy: 0.1039 - val_loss: 3.2617\n",
      "Epoch 89/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0786 - loss: 3.1555 - val_accuracy: 0.1039 - val_loss: 3.2593\n",
      "Epoch 90/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.0889 - loss: 3.1532 - val_accuracy: 0.1039 - val_loss: 3.2604\n",
      "Epoch 91/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0889 - loss: 3.1561 - val_accuracy: 0.1039 - val_loss: 3.2681\n",
      "Epoch 92/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.0640 - loss: 3.1601 - val_accuracy: 0.1039 - val_loss: 3.2607\n",
      "Epoch 93/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0938 - loss: 3.1558 - val_accuracy: 0.1039 - val_loss: 3.2707\n",
      "Epoch 94/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0985 - loss: 3.1327 - val_accuracy: 0.1039 - val_loss: 3.2621\n",
      "Epoch 95/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0931 - loss: 3.1543 - val_accuracy: 0.1039 - val_loss: 3.2590\n",
      "Epoch 96/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0926 - loss: 3.1413 - val_accuracy: 0.1039 - val_loss: 3.2710\n",
      "Epoch 97/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0816 - loss: 3.1502 - val_accuracy: 0.1039 - val_loss: 3.2593\n",
      "Epoch 98/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0814 - loss: 3.1509 - val_accuracy: 0.1039 - val_loss: 3.2656\n",
      "Epoch 99/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.1082 - loss: 3.1285 - val_accuracy: 0.1039 - val_loss: 3.2668\n",
      "Epoch 100/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.0784 - loss: 3.1419 - val_accuracy: 0.1039 - val_loss: 3.2654\n",
      "Final Training Accuracy: 0.0679\n",
      "Final Validation Accuracy: 0.1039\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0815 - loss: 3.1955\n",
      "Test Loss: 3.1861\n",
      "Test Accuracy: 0.0777\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         8\n",
      "           8       0.00      0.00      0.00        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00         7\n",
      "          11       0.00      0.00      0.00         6\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.08      1.00      0.14        15\n",
      "          16       0.00      0.00      0.00         8\n",
      "          17       0.00      0.00      0.00         3\n",
      "          18       0.00      0.00      0.00        12\n",
      "          19       0.00      0.00      0.00         7\n",
      "          20       0.00      0.00      0.00        10\n",
      "          21       0.00      0.00      0.00         7\n",
      "          22       0.00      0.00      0.00         8\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.08       193\n",
      "   macro avg       0.00      0.04      0.01       193\n",
      "weighted avg       0.01      0.08      0.01       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "rnn_model.compile(optimizer=sgd_optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
    "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RNN with Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 134ms/step - accuracy: 0.2494 - loss: 3.0171 - val_accuracy: 0.6234 - val_loss: 2.5586\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.6748 - loss: 2.1932 - val_accuracy: 0.7143 - val_loss: 1.9266\n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.7559 - loss: 1.4965 - val_accuracy: 0.7922 - val_loss: 1.5176\n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8021 - loss: 1.1339 - val_accuracy: 0.8182 - val_loss: 1.1271\n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.8883 - loss: 0.7577 - val_accuracy: 0.8442 - val_loss: 0.8956\n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8738 - loss: 0.6838 - val_accuracy: 0.8701 - val_loss: 0.6662\n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9130 - loss: 0.4693 - val_accuracy: 0.8961 - val_loss: 0.4993\n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9436 - loss: 0.3205 - val_accuracy: 0.9091 - val_loss: 0.4124\n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9489 - loss: 0.3129 - val_accuracy: 0.8701 - val_loss: 0.4614\n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9207 - loss: 0.3240 - val_accuracy: 0.9351 - val_loss: 0.3678\n",
      "Final Training Accuracy: 0.9292\n",
      "Final Validation Accuracy: 0.9351\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9503 - loss: 0.3252\n",
      "Test Loss: 0.3650\n",
      "Test Accuracy: 0.9378\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40         3\n",
      "           1       0.46      1.00      0.63         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      0.67      0.80         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      0.60      0.75         5\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       0.89      0.67      0.76        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       0.94      1.00      0.97        15\n",
      "          16       1.00      1.00      1.00         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       1.00      1.00      1.00         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       1.00      1.00      1.00        16\n",
      "          24       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.94       193\n",
      "   macro avg       0.94      0.93      0.92       193\n",
      "weighted avg       0.96      0.94      0.94       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(resumeDataSet['Category'])\n",
    "encoded_labels = le.transform(resumeDataSet['Category'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RNN model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    SimpleRNN(64, return_sequences=False),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_rnn = history_rnn.history['accuracy'][-1]\n",
    "val_accuracy_rnn = history_rnn.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_rnn:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_rnn, test_accuracy_rnn = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_rnn:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rnn:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rnn = np.argmax(rnn_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(resumeDataSet['Category'])\n",
    "encoded_labels = le.transform(resumeDataSet['Category'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
    "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Ensure padded_sequences is defined\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(padded_sequences, resumeDataSet['Category'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
    "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Get the original resume text corresponding to the test set indices\n",
    "test_resume_texts = resumeDataSet['Resume'].iloc[y_test.index].tolist()  \n",
    "\n",
    "# Now use this list of texts for tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_resume_texts)  \n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                    loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m                    metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m history_lstm \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mpadded_sequences\u001b[49m, resumeDataSet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Retrieve training and validation accuracy from history\u001b[39;00m\n\u001b[0;32m     22\u001b[0m train_accuracy_lstm \u001b[38;5;241m=\u001b[39m history_lstm\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'padded_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Ensure padded_sequences is defined\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = lstm_model.fit(padded_sequences, resumeDataSet['Category'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm = history_lstm.history['accuracy'][-1]\n",
    "val_accuracy_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Get the original resume text corresponding to the test set indices\n",
    "test_resume_texts = resumeDataSet['Resume'].iloc[y_test.index].tolist()  \n",
    "\n",
    "# Now use this list of texts for tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_resume_texts)  \n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "loss_lstm, test_accuracy_lstm = lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss: {loss_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM with GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 113ms/step - accuracy: 0.0458 - loss: 3.2169 - val_accuracy: 0.0000e+00 - val_loss: 3.3401\n",
      "Epoch 2/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.1071 - loss: 3.1881 - val_accuracy: 0.0000e+00 - val_loss: 3.4938\n",
      "Epoch 3/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.0984 - loss: 3.1647 - val_accuracy: 0.0000e+00 - val_loss: 3.6221\n",
      "Epoch 4/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.0867 - loss: 3.1649 - val_accuracy: 0.0000e+00 - val_loss: 3.7305\n",
      "Epoch 5/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.0996 - loss: 3.1339 - val_accuracy: 0.0000e+00 - val_loss: 3.8851\n",
      "Epoch 6/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.0927 - loss: 3.1337 - val_accuracy: 0.0000e+00 - val_loss: 4.0037\n",
      "Epoch 7/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.0909 - loss: 3.1274 - val_accuracy: 0.0000e+00 - val_loss: 4.1201\n",
      "Epoch 8/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.0830 - loss: 3.1248 - val_accuracy: 0.0000e+00 - val_loss: 4.2171\n",
      "Epoch 9/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.0979 - loss: 3.0922 - val_accuracy: 0.0000e+00 - val_loss: 4.2936\n",
      "Epoch 10/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.0908 - loss: 3.0685 - val_accuracy: 0.0000e+00 - val_loss: 4.4071\n",
      "Epoch 11/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.0974 - loss: 3.0434 - val_accuracy: 0.0000e+00 - val_loss: 4.5329\n",
      "Epoch 12/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.1081 - loss: 3.0181 - val_accuracy: 0.0000e+00 - val_loss: 4.5891\n",
      "Epoch 13/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.1096 - loss: 2.9966 - val_accuracy: 0.0000e+00 - val_loss: 4.6957\n",
      "Epoch 14/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.1225 - loss: 2.9650 - val_accuracy: 0.0000e+00 - val_loss: 4.7838\n",
      "Epoch 15/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.0929 - loss: 2.9747 - val_accuracy: 0.0000e+00 - val_loss: 4.8948\n",
      "Epoch 16/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.1199 - loss: 2.9170 - val_accuracy: 0.0000e+00 - val_loss: 4.9932\n",
      "Epoch 17/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.1249 - loss: 2.9426 - val_accuracy: 0.0000e+00 - val_loss: 5.0872\n",
      "Epoch 18/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.1196 - loss: 2.9224 - val_accuracy: 0.0000e+00 - val_loss: 5.2365\n",
      "Epoch 19/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.1106 - loss: 2.9018 - val_accuracy: 0.0000e+00 - val_loss: 5.1869\n",
      "Epoch 20/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.1362 - loss: 2.8940 - val_accuracy: 0.0000e+00 - val_loss: 5.3183\n",
      "Epoch 21/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.1166 - loss: 2.8574 - val_accuracy: 0.0000e+00 - val_loss: 5.4278\n",
      "Epoch 22/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.1553 - loss: 2.8366 - val_accuracy: 0.0000e+00 - val_loss: 5.5394\n",
      "Epoch 23/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.2180 - loss: 2.7756 - val_accuracy: 0.0000e+00 - val_loss: 5.6118\n",
      "Epoch 24/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.1949 - loss: 2.7633 - val_accuracy: 0.0000e+00 - val_loss: 5.7106\n",
      "Epoch 25/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.1716 - loss: 2.6961 - val_accuracy: 0.0000e+00 - val_loss: 5.7693\n",
      "Epoch 26/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.2304 - loss: 2.5406 - val_accuracy: 0.0000e+00 - val_loss: 5.7575\n",
      "Epoch 27/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.2690 - loss: 2.4692 - val_accuracy: 0.0000e+00 - val_loss: 6.0091\n",
      "Epoch 28/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.3251 - loss: 2.2708 - val_accuracy: 0.0000e+00 - val_loss: 6.2029\n",
      "Epoch 29/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.3238 - loss: 2.2152 - val_accuracy: 0.0000e+00 - val_loss: 5.8577\n",
      "Epoch 30/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.2537 - loss: 2.4538 - val_accuracy: 0.0000e+00 - val_loss: 5.6433\n",
      "Epoch 31/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.3445 - loss: 2.0836 - val_accuracy: 0.0000e+00 - val_loss: 6.1600\n",
      "Epoch 32/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.3791 - loss: 1.9556 - val_accuracy: 0.0000e+00 - val_loss: 6.6824\n",
      "Epoch 33/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.4299 - loss: 1.7963 - val_accuracy: 0.0000e+00 - val_loss: 7.1311\n",
      "Epoch 34/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.4165 - loss: 1.7881 - val_accuracy: 0.0000e+00 - val_loss: 7.4057\n",
      "Epoch 35/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.4762 - loss: 1.6543 - val_accuracy: 0.0000e+00 - val_loss: 7.4966\n",
      "Epoch 36/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - accuracy: 0.4557 - loss: 1.6611 - val_accuracy: 0.0000e+00 - val_loss: 6.8829\n",
      "Epoch 37/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.3742 - loss: 1.8146 - val_accuracy: 0.0000e+00 - val_loss: 6.9754\n",
      "Epoch 38/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.4439 - loss: 1.7014 - val_accuracy: 0.0000e+00 - val_loss: 7.6303\n",
      "Epoch 39/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.4768 - loss: 1.5392 - val_accuracy: 0.0000e+00 - val_loss: 7.7129\n",
      "Epoch 40/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.5209 - loss: 1.4834 - val_accuracy: 0.0000e+00 - val_loss: 7.8984\n",
      "Epoch 41/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.5553 - loss: 1.4120 - val_accuracy: 0.0000e+00 - val_loss: 8.1192\n",
      "Epoch 42/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.5722 - loss: 1.3477 - val_accuracy: 0.0000e+00 - val_loss: 8.4277\n",
      "Epoch 43/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.5707 - loss: 1.3199 - val_accuracy: 0.0000e+00 - val_loss: 9.1459\n",
      "Epoch 44/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.5961 - loss: 1.2415 - val_accuracy: 0.0000e+00 - val_loss: 8.6339\n",
      "Epoch 45/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.5919 - loss: 1.2101 - val_accuracy: 0.0000e+00 - val_loss: 8.6002\n",
      "Epoch 46/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.6890 - loss: 1.0741 - val_accuracy: 0.0000e+00 - val_loss: 9.5780\n",
      "Epoch 47/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.6101 - loss: 1.1387 - val_accuracy: 0.0000e+00 - val_loss: 10.0908\n",
      "Epoch 48/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.5468 - loss: 1.4063 - val_accuracy: 0.0000e+00 - val_loss: 9.0430\n",
      "Epoch 49/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.5988 - loss: 1.1262 - val_accuracy: 0.0000e+00 - val_loss: 8.7086\n",
      "Epoch 50/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.7209 - loss: 0.9431 - val_accuracy: 0.0000e+00 - val_loss: 10.6636\n",
      "Epoch 51/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.6727 - loss: 0.9815 - val_accuracy: 0.0000e+00 - val_loss: 9.5387\n",
      "Epoch 52/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.7796 - loss: 0.7634 - val_accuracy: 0.0000e+00 - val_loss: 9.9851\n",
      "Epoch 53/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.7859 - loss: 0.7544 - val_accuracy: 0.0000e+00 - val_loss: 10.3424\n",
      "Epoch 54/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8443 - loss: 0.6138 - val_accuracy: 0.0515 - val_loss: 10.7435\n",
      "Epoch 55/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8821 - loss: 0.5177 - val_accuracy: 0.0000e+00 - val_loss: 10.4709\n",
      "Epoch 56/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8559 - loss: 0.5409 - val_accuracy: 0.0000e+00 - val_loss: 11.0131\n",
      "Epoch 57/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.8900 - loss: 0.4480 - val_accuracy: 0.1134 - val_loss: 10.7369\n",
      "Epoch 58/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.8483 - loss: 0.5379 - val_accuracy: 0.1134 - val_loss: 11.4116\n",
      "Epoch 59/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9043 - loss: 0.4202 - val_accuracy: 0.0619 - val_loss: 11.1511\n",
      "Epoch 60/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9012 - loss: 0.4272 - val_accuracy: 0.1134 - val_loss: 11.3436\n",
      "Epoch 61/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8938 - loss: 0.3762 - val_accuracy: 0.1134 - val_loss: 11.8350\n",
      "Epoch 62/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9259 - loss: 0.3532 - val_accuracy: 0.1134 - val_loss: 11.9295\n",
      "Epoch 63/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.9648 - loss: 0.2448 - val_accuracy: 0.2784 - val_loss: 12.6390\n",
      "Epoch 64/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9827 - loss: 0.1840 - val_accuracy: 0.2784 - val_loss: 12.2919\n",
      "Epoch 65/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9572 - loss: 0.1979 - val_accuracy: 0.2784 - val_loss: 13.0381\n",
      "Epoch 66/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9627 - loss: 0.1842 - val_accuracy: 0.2784 - val_loss: 13.2513\n",
      "Epoch 67/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9821 - loss: 0.1562 - val_accuracy: 0.2784 - val_loss: 13.1154\n",
      "Epoch 68/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.9738 - loss: 0.1306 - val_accuracy: 0.2784 - val_loss: 13.3742\n",
      "Epoch 69/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9858 - loss: 0.1126 - val_accuracy: 0.2784 - val_loss: 13.8756\n",
      "Epoch 70/70\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.9875 - loss: 0.0908 - val_accuracy: 0.2784 - val_loss: 13.6714\n",
      "Final Training Accuracy with SGD: 0.9873\n",
      "Final Validation Accuracy with SGD: 0.2784\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9050 - loss: 1.5307\n",
      "Test Loss with SGD: 1.6344\n",
      "Test Accuracy with SGD: 0.9016\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.38      1.00      0.56         5\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       0.89      1.00      0.94         8\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       0.88      1.00      0.93         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       0.86      1.00      0.92        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00        15\n",
      "          16       0.73      1.00      0.84         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.70      1.00      0.82         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.90       193\n",
      "   macro avg       0.89      0.93      0.90       193\n",
      "weighted avg       0.86      0.90      0.87       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "lstm_model.compile(optimizer=sgd_optimizer,\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm_sgd = lstm_model.fit(padded_sequences, resumeDataSet['Category'], epochs=70, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_lstm_sgd = history_lstm_sgd.history['accuracy'][-1]\n",
    "val_accuracy_lstm_sgd = history_lstm_sgd.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy with SGD: {train_accuracy_lstm_sgd:.4f}\")\n",
    "print(f\"Final Validation Accuracy with SGD: {val_accuracy_lstm_sgd:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_lstm_sgd, test_accuracy_lstm_sgd = lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss with SGD: {loss_lstm_sgd:.4f}\")\n",
    "print(f\"Test Accuracy with SGD: {test_accuracy_lstm_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm_sgd = np.argmax(lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_lstm_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BI-LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "le.fit(resumeDataSet['Category'])\n",
    "encoded_labels = le.transform(resumeDataSet['Category'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the BI-LSTM model\n",
    "bi_lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bi_lstm_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_bi_lstm = bi_lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_bi_lstm = history_bi_lstm.history['accuracy'][-1]\n",
    "val_accuracy_bi_lstm = history_bi_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_bi_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_bi_lstm, test_accuracy_bi_lstm = bi_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_bi_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bi_lstm = np.argmax(bi_lstm_model.predict(X_test), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_bi_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.1145 - loss: 3.1771 - val_accuracy: 0.0000e+00 - val_loss: 3.6304\n",
      "Epoch 2/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.1657 - loss: 2.8476 - val_accuracy: 0.0000e+00 - val_loss: 4.4476\n",
      "Epoch 3/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.4423 - loss: 2.1083 - val_accuracy: 0.0000e+00 - val_loss: 5.7524\n",
      "Epoch 4/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.7394 - loss: 1.1998 - val_accuracy: 0.0000e+00 - val_loss: 6.5063\n",
      "Epoch 5/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9042 - loss: 0.6566 - val_accuracy: 0.0515 - val_loss: 6.8324\n",
      "Epoch 6/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - accuracy: 0.9592 - loss: 0.4018 - val_accuracy: 0.2784 - val_loss: 6.7604\n",
      "Epoch 7/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9878 - loss: 0.2112 - val_accuracy: 0.2784 - val_loss: 7.7011\n",
      "Epoch 8/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9999 - loss: 0.1087 - val_accuracy: 0.2784 - val_loss: 8.0691\n",
      "Epoch 9/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 0.0755 - val_accuracy: 0.2784 - val_loss: 8.4055\n",
      "Epoch 10/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.0459 - val_accuracy: 0.2784 - val_loss: 8.4923\n",
      "Final Training Accuracy: 1.0000\n",
      "Final Validation Accuracy: 0.2784\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9236 - loss: 0.9354\n",
      "Test Loss: 0.9993\n",
      "Test Accuracy: 0.9171\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       0.58      1.00      0.74         7\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         8\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       0.46      1.00      0.63         6\n",
      "          12       1.00      1.00      1.00        12\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       0.88      1.00      0.93         7\n",
      "          15       1.00      1.00      1.00        15\n",
      "          16       1.00      1.00      1.00         8\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       0.80      1.00      0.89        12\n",
      "          19       1.00      1.00      1.00         7\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         7\n",
      "          22       1.00      1.00      1.00         8\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.92       193\n",
      "   macro avg       0.91      0.96      0.93       193\n",
      "weighted avg       0.87      0.92      0.89       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(resumeDataSet['Resume'])\n",
    "sequences = tokenizer.texts_to_sequences(resumeDataSet['Resume'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Define the BI-LSTM model\n",
    "bi_lstm_model = keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bi_lstm_model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_bi_lstm = bi_lstm_model.fit(padded_sequences, resumeDataSet['Category'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_bi_lstm = history_bi_lstm.history['accuracy'][-1]\n",
    "val_accuracy_bi_lstm = history_bi_lstm.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy_bi_lstm:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "# Get the original resume text corresponding to the test set indices\n",
    "test_resume_texts = resumeDataSet['Resume'].iloc[y_test.index].tolist()  \n",
    "\n",
    "# Now use this list of texts for tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_resume_texts)  \n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "loss_bi_lstm, test_accuracy_bi_lstm = bi_lstm_model.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss: {loss_bi_lstm:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_bi_lstm:.4f}\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bi_lstm = np.argmax(bi_lstm_model.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_bi_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BI-LSTM with GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 134ms/step - accuracy: 0.0657 - loss: 3.2117 - val_accuracy: 0.0000e+00 - val_loss: 3.3698\n",
      "Epoch 2/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.0936 - loss: 3.1778 - val_accuracy: 0.0000e+00 - val_loss: 3.5718\n",
      "Epoch 3/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.1052 - loss: 3.1500 - val_accuracy: 0.0000e+00 - val_loss: 3.7622\n",
      "Epoch 4/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.1086 - loss: 3.1333 - val_accuracy: 0.0000e+00 - val_loss: 3.9132\n",
      "Epoch 5/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.1094 - loss: 3.1167 - val_accuracy: 0.0000e+00 - val_loss: 4.0698\n",
      "Epoch 6/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.0926 - loss: 3.1298 - val_accuracy: 0.0000e+00 - val_loss: 4.2260\n",
      "Epoch 7/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.1048 - loss: 3.1105 - val_accuracy: 0.0000e+00 - val_loss: 4.2680\n",
      "Epoch 8/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.0758 - loss: 3.1155 - val_accuracy: 0.0000e+00 - val_loss: 4.3698\n",
      "Epoch 9/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.1079 - loss: 3.0686 - val_accuracy: 0.0000e+00 - val_loss: 4.4695\n",
      "Epoch 10/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.0955 - loss: 3.0687 - val_accuracy: 0.0000e+00 - val_loss: 4.5853\n",
      "Final Training Accuracy with SGD: 0.0971\n",
      "Final Validation Accuracy with SGD: 0.0000\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0815 - loss: 3.2069\n",
      "Test Loss with SGD: 3.2266\n",
      "Test Accuracy with SGD: 0.0777\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         8\n",
      "           8       0.00      0.00      0.00        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00         7\n",
      "          11       0.00      0.00      0.00         6\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.08      1.00      0.14        15\n",
      "          16       0.00      0.00      0.00         8\n",
      "          17       0.00      0.00      0.00         3\n",
      "          18       0.00      0.00      0.00        12\n",
      "          19       0.00      0.00      0.00         7\n",
      "          20       0.00      0.00      0.00        10\n",
      "          21       0.00      0.00      0.00         7\n",
      "          22       0.00      0.00      0.00         8\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.08       193\n",
      "   macro avg       0.00      0.04      0.01       193\n",
      "weighted avg       0.01      0.08      0.01       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the BI-LSTM model\n",
    "bi_lstm_model_sgd = keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=200),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer\n",
    "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "bi_lstm_model_sgd.compile(optimizer=sgd_optimizer,\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_bi_lstm_sgd = bi_lstm_model_sgd.fit(padded_sequences, resumeDataSet['Category'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Retrieve training and validation accuracy from history\n",
    "train_accuracy_bi_lstm_sgd = history_bi_lstm_sgd.history['accuracy'][-1]\n",
    "val_accuracy_bi_lstm_sgd = history_bi_lstm_sgd.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"Final Training Accuracy with SGD: {train_accuracy_bi_lstm_sgd:.4f}\")\n",
    "print(f\"Final Validation Accuracy with SGD: {val_accuracy_bi_lstm_sgd:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_bi_lstm_sgd, test_accuracy_bi_lstm_sgd = bi_lstm_model_sgd.evaluate(padded_test_sequences, y_test)\n",
    "print(f\"Test Loss with SGD: {loss_bi_lstm_sgd:.4f}\")\n",
    "print(f\"Test Accuracy with SGD: {test_accuracy_bi_lstm_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bi_lstm_sgd = np.argmax(bi_lstm_model_sgd.predict(padded_test_sequences), axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_bi_lstm_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def _init_(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels= 25)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            labels = d['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch( model, train_loader, loss_fn, optimizer, device, None, len(train_dataset))\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    test_acc, test_loss = eval_model(model, test_loader, loss_fn, device, len(test_dataset))\n",
    "\n",
    "    print(f'Test loss {test_loss} accuracy {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Deployment:** The trained models, along with the TF-IDF vectorizer and label encoder, are serialized (pickled) for use in the Streamlit application. This step makes the models reusable without retraining each time, **and it is where Streamlit comes into play**. This app was built using Streamlit, a python library to build interactive and sharable web applications. Streamlit made it easier to build this application by handling the user interface, input/output and allowing to display the predictions of different models to be accessible through a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('tfidf.pkl','wb'))\n",
    "pickle.dump(knn_model, open('knn.pkl', 'wb'))\n",
    "pickle.dump(svc_model, open('svc.pkl', 'wb'))\n",
    "pickle.dump(rf_model, open('rf.pkl', 'wb'))\n",
    "pickle.dump(model, open('mlp.pkl', 'wb'))\n",
    "pickle.dump(ensemble_model, open('ensemble.pkl', 'wb'))\n",
    "pickle.dump(le, open(\"encoder.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction:** The app provides predictions from all the trained models, allowing for an extensive comparison of their results. The user receives a result from each model, including KNN, SVC, Random Forest, MLP, and the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the category of a resume and print results for each model\n",
    "def pred(input_resume):\n",
    "    # Preprocess the input text (e.g., cleaning, etc.)\n",
    "    cleaned_text = cleanResume(input_resume)\n",
    "\n",
    "    # Vectorize the cleaned text using the same TF-IDF vectorizer used during training\n",
    "    vectorized_text = tfidf.transform([cleaned_text])\n",
    "\n",
    "    # Convert sparse matrix to dense\n",
    "    vectorized_text = vectorized_text.toarray()\n",
    "\n",
    "    # Prediction\n",
    "    predicted_category_knn = knn_model.predict(vectorized_text)\n",
    "    predicted_category_svc = svc_model.predict(vectorized_text) \n",
    "    predicted_category_rf = rf_model.predict(vectorized_text)\n",
    "    predicted_category_mlp = np.argmax(model.predict(vectorized_text), axis=1)\n",
    "    predicted_category_ensemble = ensemble_model.predict(vectorized_text)\n",
    "\n",
    "    # Get name of predicted category for each model\n",
    "    category_knn = le.inverse_transform(predicted_category_knn)[0]\n",
    "    category_svc = le.inverse_transform(predicted_category_svc)[0]\n",
    "    category_rf = le.inverse_transform(predicted_category_rf)[0]\n",
    "    category_mlp = le.inverse_transform(predicted_category_mlp)[0]\n",
    "    category_ensemble = le.inverse_transform(predicted_category_ensemble)[0]\n",
    "\n",
    "    # Print results for each model\n",
    "    print(f\"KNN Model Prediction: {category_knn}\")\n",
    "    print(f\"SVC Model Prediction: {category_svc}\")\n",
    "    print(f\"Random Forest Model Prediction: {category_rf}\")\n",
    "    print(f\"MLP Model Prediction: {category_mlp}\")\n",
    "    print(f\"Ensemble Model Prediction: {category_ensemble}\")\n",
    "\n",
    "    # Return the category name predicted by the first model (as an example)\n",
    "    return category_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "KNN Model Prediction: HR\n",
      "SVC Model Prediction: HR\n",
      "Random Forest Model Prediction: HR\n",
      "MLP Model Prediction: HR\n",
      "Ensemble Model Prediction: HR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HR'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume = \"\"\"Name: Sarah Johnson\n",
    "Contact Information:\n",
    "\n",
    "Phone: +1 555-123-4567\n",
    "Email: sarah.johnson@email.com\n",
    "LinkedIn: linkedin.com/in/sarahjohnson\n",
    "Address: New York, NY\n",
    "Professional Summary\n",
    "A results-driven HR professional with over 6 years of experience in talent acquisition, employee engagement, and HR operations. Skilled in building strong teams, fostering positive work environments, and implementing HR strategies that align with organizational goals. Proficient in HR software and data-driven decision-making to improve workforce management.\n",
    "\n",
    "Key Skills\n",
    "Talent Acquisition and Recruitment\n",
    "Employee Onboarding and Training\n",
    "Performance Management\n",
    "HR Policies and Compliance\n",
    "Employee Relations and Engagement\n",
    "Compensation and Benefits Administration\n",
    "HR Analytics and Reporting\n",
    "HR Software: SAP, Workday, BambooHR\n",
    "Strong Interpersonal and Communication Skills\n",
    "Professional Experience\n",
    "HR Manager\n",
    "BrightPath Solutions | January 2020 – Present\n",
    "\n",
    "Led end-to-end recruitment processes, successfully hiring over 50 candidates annually across various roles.\n",
    "Designed and implemented onboarding programs, reducing new hire turnover by 20%.\n",
    "Developed performance management frameworks, increasing employee productivity by 15%.\n",
    "Conducted employee satisfaction surveys and implemented strategies to enhance engagement.\n",
    "Ensured compliance with labor laws and company policies, minimizing legal risks.\n",
    "HR Generalist\n",
    "Global Reach Inc. | June 2016 – December 2019\n",
    "\n",
    "Supported HR operations, including recruitment, payroll, and benefits administration.\n",
    "Assisted in developing HR policies and communicated updates to employees.\n",
    "Resolved employee grievances, fostering a collaborative workplace.\n",
    "Analyzed HR metrics to identify trends and presented actionable insights to management.\n",
    "HR Coordinator\n",
    "TalentFirst Consulting | March 2014 – May 2016\n",
    "\n",
    "Scheduled interviews and coordinated recruitment activities.\n",
    "Maintained employee records and ensured accuracy in HR databases.\n",
    "Assisted in planning company events and training sessions.\n",
    "Education\n",
    "Bachelor’s Degree in Human Resource Management\n",
    "University of California, Berkeley | 2013\n",
    "\n",
    "Certifications\n",
    "\n",
    "Certified Professional in Human Resources (PHR)\n",
    "SHRM Certified Professional (SHRM-CP)\n",
    "Advanced HR Analytics (Coursera)\n",
    "Achievements\n",
    "Reduced time-to-hire by 30% through process optimization.\n",
    "Increased employee retention by 25% by implementing a mentorship program.\n",
    "Spearheaded diversity and inclusion initiatives, leading to a 40% increase in diverse hires.\n",
    "Languages\n",
    "English (Fluent)\n",
    "Spanish (Intermediate)\n",
    "\"\"\"\n",
    "\n",
    "pred(myresume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "KNN Model Prediction: Data Science\n",
      "SVC Model Prediction: Data Science\n",
      "Random Forest Model Prediction: Data Science\n",
      "MLP Model Prediction: Data Science\n",
      "Ensemble Model Prediction: Data Science\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresume = \"\"\"I am a Business Analyst specializing in developing dashboards, \n",
    "reports, and data models to drive performance insights. \n",
    "Proficient in Python, R, SQL, Excel, and Power BI, I excel in data \n",
    "analysis, advanced analytics, and automation of data processes. \n",
    "Skilled in statistical analysis and data visualization, I derive \n",
    "insights for data-driven decisions. Experienced in designing and \n",
    "optimizing data warehouse solutions, managing ETL processes, \n",
    "and ensuring data integrity and security. Additionally, I hold a \n",
    "CCNA certification from Cisco, showcasing my knowledge in \n",
    "networking.\n",
    "\"\"\"\n",
    "\n",
    "pred(myresume)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
